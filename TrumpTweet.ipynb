{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TrumpTweet Notebook\n",
    "This notebook references the code to create a Trump tweet. This includes:\n",
    "- Processing the Trump Tweet archive to create a clean file of Trump's tweets\n",
    "- Defining and training GRU long short term memory recurrent neural network\n",
    "- Feeding data into the model in order to create a novel tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the helpers scripts with the data and model helper objects\n",
    "%run scripts/helpers\n",
    "#from scripts import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify GPU support\n",
    "tf.config.list_physical_devices('GPU')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a datahelper and process raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete.\n",
      "The number of Tweets sent by Trump during the period is 2362\n"
     ]
    }
   ],
   "source": [
    "# Create a datahelper object and designate the input file\n",
    "dh = DataHelper(file_name='tweets_11-06-2020.csv')\n",
    "\n",
    "# Prep the raw data to create the tweet file\n",
    "dh.prep_raw_data(start_date='2020-08-01', end_date='2020-11-19')\n",
    "\n",
    "# Print the number of tweets\n",
    "print('The number of Tweets sent by Trump during the period is {}'.format(dh.num_tweets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize the text and create the dataset for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function DataHelper.create_tokenizer.<locals>.<lambda> at 0x0000018A67682168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'NoneType' object has no attribute '__dict__'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function DataHelper.create_tokenizer.<locals>.<lambda> at 0x0000018A67682168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'NoneType' object has no attribute '__dict__'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function DataHelper.create_tokenizer.<locals>.<lambda> at 0x0000018A67682438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'NoneType' object has no attribute '__dict__'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function DataHelper.create_tokenizer.<locals>.<lambda> at 0x0000018A67682438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'NoneType' object has no attribute '__dict__'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function DataHelper.create_tokenizer.<locals>.<lambda> at 0x0000018A676823A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'NoneType' object has no attribute '__dict__'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function DataHelper.create_tokenizer.<locals>.<lambda> at 0x0000018A676823A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'NoneType' object has no attribute '__dict__'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Dataset and tokenizer creation complete.\n",
      "The number of unique characters is 75 and the dataset size is 1 document(s). The number of windows in the dataset for processing is 4,379,204.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text and create the dataset\n",
    "dataset, tokenizer = dh.create_tokenizer('inputdata/clean_tweet.txt')\n",
    "print('The number of unique characters is {0:,} and the dataset size is {1:,} document(s).' \\\n",
    "      ' The number of windows in the dataset for processing is {2:,}.'.format(dh.num_unique_chars,dh.dataset_size,dh.num_data_windows))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the Model and Train It\n",
    "The model is a stateless RNN made up of GRU cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "136851/136851 [==============================] - 10397s 76ms/step - loss: 1.3428\n",
      "Epoch 2/20\n",
      "136851/136851 [==============================] - 10362s 76ms/step - loss: 1.1128\n",
      "Epoch 3/20\n",
      "136851/136851 [==============================] - 10386s 76ms/step - loss: 1.0804\n",
      "Epoch 4/20\n",
      "136851/136851 [==============================] - 10365s 76ms/step - loss: 1.0646\n",
      "Epoch 5/20\n",
      "136851/136851 [==============================] - 10365s 76ms/step - loss: 1.0524\n",
      "Epoch 6/20\n",
      "136851/136851 [==============================] - 10364s 76ms/step - loss: 1.0425\n",
      "Epoch 7/20\n",
      " 83734/136851 [=================>............] - ETA: 1:07:00 - loss: 1.0354"
     ]
    }
   ],
   "source": [
    "# Create the modelhelper object\n",
    "mh = ModelHelper(epochs=20)\n",
    "\n",
    "# Create the model\n",
    "model = mh.create_model(tokenizer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam')\n",
    "\n",
    "# Save a checkpoint after every epoch\n",
    "EPOCHS = 20\n",
    "checkpoint_filepath = 'checkpoints/weights.{epoch:02d}.hdf5'\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    save_freq='epoch',\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=False)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(dataset,epochs=EPOCHS,callbacks=[model_checkpoint])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Start Training Model from Saved Checkpoint\n",
    "Training the model takes quite some time. A checkpoint is saved every epoch so the code below will allow you to resume training from a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#restart training from saved checkpoint\n",
    "new_model = load_model('checkpoints/weights.20.model1.hdf5')\n",
    "\n",
    "# Save a checkpoint after every epoch\n",
    "EPOCHS = 20\n",
    "checkpoint_filepath = 'checkpoints/weights.{epoch:02d}.hdf5'\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    save_freq='epoch',\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=False)\n",
    "\n",
    "#Fit the model\n",
    "history = new_model.fit(dataset,epochs=EPOCHS,callbacks=[model_checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate some tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a long sequence of text\n",
    "print(mh.create_tweet('election', temperature=0.2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
