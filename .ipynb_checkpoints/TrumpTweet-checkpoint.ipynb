{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TrumpTweet Notebook\n",
    "This notebook references the code to create a Trump tweet. This includes:\n",
    "- Processing the Trump Tweet archive to create a clean file of Trump's tweets\n",
    "- Defining and training GRU long short term memory recurrent neural network\n",
    "- Feeding data into the model in order to create a novel tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the helpers scripts with the data and model helper objects\n",
    "%run scripts/helpers\n",
    "#from scripts import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify GPU support\n",
    "tf.config.list_physical_devices('GPU')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a datahelper and process raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 56090 entries, 0 to 56089\n",
      "Data columns (total 8 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         56090 non-null  int64 \n",
      " 1   text       56090 non-null  object\n",
      " 2   isRetweet  56090 non-null  object\n",
      " 3   isDeleted  56090 non-null  object\n",
      " 4   device     56090 non-null  object\n",
      " 5   favorites  56090 non-null  int64 \n",
      " 6   retweets   56090 non-null  int64 \n",
      " 7   date       56090 non-null  object\n",
      "dtypes: int64(3), object(5)\n",
      "memory usage: 3.4+ MB\n",
      "None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>=' not supported between instances of 'datetime.datetime' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-94a53138eac9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Prep the raw data to create the tweet file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprep_raw_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'2020-06-01'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'2020-12-29'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Print the number of tweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\TrumpTweet\\scripts\\helpers.py\u001b[0m in \u001b[0;36mprep_raw_data\u001b[1;34m(self, start_date, end_date)\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;31m#Get the number of tweets sent by Trump himself - not the retweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mdf_tt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_tt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_tt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'isRetweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'f'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[0mdf_tt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_tt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_tt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbetween\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mend_date\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_tweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_tt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras_env\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mbetween\u001b[1;34m(self, left, right, inclusive)\u001b[0m\n\u001b[0;32m   4755\u001b[0m         \"\"\"\n\u001b[0;32m   4756\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minclusive\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4757\u001b[1;33m             \u001b[0mlmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mleft\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4758\u001b[0m             \u001b[0mrmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4759\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras_env\\lib\\site-packages\\pandas\\core\\ops\\common.py\u001b[0m in \u001b[0;36mnew_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras_env\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[0mrvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 370\u001b[1;33m         \u001b[0mres_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomparison_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras_env\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001b[0m in \u001b[0;36mcomparison_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m         \u001b[0mres_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomp_method_OBJECT_ARRAY\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras_env\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001b[0m in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[1;34m(op, x, y)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvec_compare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscalar_compare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\ops.pyx\u001b[0m in \u001b[0;36mpandas._libs.ops.scalar_compare\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '>=' not supported between instances of 'datetime.datetime' and 'str'"
     ]
    }
   ],
   "source": [
    "# Create a datahelper object and designate the input file\n",
    "dh = DataHelper(file_name='tweets_12-29-2020.csv')\n",
    "\n",
    "# Prep the raw data to create the tweet file\n",
    "dh.prep_raw_data(start_date='2020-06-01', end_date='2020-12-29')\n",
    "\n",
    "# Print the number of tweets\n",
    "print('The number of Tweets sent by Trump during the period is {}'.format(dh.num_tweets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize the text and create the dataset for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset and tokenizer creation complete.\n",
      "The number of unique characters is 75 and the dataset size is 1 document(s). The number of windows in the dataset for processing is 4,926,622.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text and create the dataset\n",
    "dataset, tokenizer = dh.create_tokenizer('inputdata/clean_tweet.txt')\n",
    "print('The number of unique characters is {0:,} and the dataset size is {1:,} document(s).' \\\n",
    "      ' The number of windows in the dataset for processing is {2:,}.'.format(dh.num_unique_chars,dh.dataset_size,dh.num_data_windows))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the Model and Train It\n",
    "The model is a stateless RNN made up of GRU cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "136851/136851 [==============================] - 10397s 76ms/step - loss: 1.3428\n",
      "Epoch 2/20\n",
      "136851/136851 [==============================] - 10362s 76ms/step - loss: 1.1128\n",
      "Epoch 3/20\n",
      "136851/136851 [==============================] - 10386s 76ms/step - loss: 1.0804\n",
      "Epoch 4/20\n",
      "136851/136851 [==============================] - 10365s 76ms/step - loss: 1.0646\n",
      "Epoch 5/20\n",
      "136851/136851 [==============================] - 10365s 76ms/step - loss: 1.0524\n",
      "Epoch 6/20\n",
      "136851/136851 [==============================] - 10364s 76ms/step - loss: 1.0425\n",
      "Epoch 7/20\n",
      "136851/136851 [==============================] - 10359s 76ms/step - loss: 1.0354\n",
      "Epoch 8/20\n",
      "136851/136851 [==============================] - 10370s 76ms/step - loss: 1.0297\n",
      "Epoch 9/20\n",
      "136851/136851 [==============================] - 10370s 76ms/step - loss: 1.0240\n",
      "Epoch 10/20\n",
      "136851/136851 [==============================] - 10381s 76ms/step - loss: nan\n",
      "Epoch 11/20\n",
      "136851/136851 [==============================] - 10363s 76ms/step - loss: nan\n",
      "Epoch 12/20\n",
      "136851/136851 [==============================] - 10352s 76ms/step - loss: nan\n",
      "Epoch 13/20\n",
      "136851/136851 [==============================] - 10347s 76ms/step - loss: nan\n",
      "Epoch 14/20\n",
      "136851/136851 [==============================] - 10362s 76ms/step - loss: nan\n",
      "Epoch 15/20\n",
      "136851/136851 [==============================] - 10358s 76ms/step - loss: nan\n",
      "Epoch 16/20\n",
      "136851/136851 [==============================] - 10383s 76ms/step - loss: nan\n",
      "Epoch 17/20\n",
      "136851/136851 [==============================] - 10367s 76ms/step - loss: nan\n",
      "Epoch 18/20\n",
      "136851/136851 [==============================] - 10366s 76ms/step - loss: nan\n",
      "Epoch 19/20\n",
      "136851/136851 [==============================] - 10339s 76ms/step - loss: nan\n",
      "Epoch 20/20\n",
      "136851/136851 [==============================] - 10363s 76ms/step - loss: nan\n"
     ]
    }
   ],
   "source": [
    "# Create the modelhelper object\n",
    "mh = ModelHelper(epochs=20)\n",
    "\n",
    "# Create the model\n",
    "model = mh.create_model(tokenizer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam')\n",
    "\n",
    "# Save a checkpoint after every epoch\n",
    "EPOCHS = 20\n",
    "checkpoint_filepath = 'checkpoints/weights.{epoch:02d}.hdf5'\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    save_freq='epoch',\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=False)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(dataset,epochs=EPOCHS,callbacks=[model_checkpoint])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Start Training Model from Saved Checkpoint\n",
    "Training the model takes quite some time. A checkpoint is saved every epoch so the code below will allow you to resume training from a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#restart training from saved checkpoint\n",
    "new_model = load_model('checkpoints/weights.20.hdf5')\n",
    "\n",
    "# Save a checkpoint after every epoch\n",
    "EPOCHS = 1\n",
    "checkpoint_filepath = 'checkpoints/weights.{epoch:02d}.hdf5'\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    save_freq='epoch',\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=False)\n",
    "\n",
    "#Fit the model\n",
    "history = new_model.fit(dataset,epochs=EPOCHS,callbacks=[model_checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a saved model and generate a Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete.\n",
      "The number of Tweets sent by Trump during the period is 2362\n"
     ]
    }
   ],
   "source": [
    "# Create a datahelper object and designate the input file\n",
    "dh = DataHelper(file_name='tweets_11-06-2020.csv')\n",
    "\n",
    "# Prep the raw data to create the tweet file\n",
    "dh.prep_raw_data(start_date='2020-08-01', end_date='2020-11-19')\n",
    "\n",
    "# Print the number of tweets\n",
    "print('The number of Tweets sent by Trump during the period is {}'.format(dh.num_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset and tokenizer creation complete.\n",
      "The number of unique characters is 75 and the dataset size is 1 document(s). The number of windows in the dataset for processing is 273,569.\n"
     ]
    }
   ],
   "source": [
    "# re-create the tokenizer\n",
    "# Tokenize the text and create the dataset\n",
    "dataset, tok = dh.create_tokenizer('inputdata/clean_tweet.txt')\n",
    "print('The number of unique characters is {0:,} and the dataset size is {1:,} document(s).' \\\n",
    "      ' The number of windows in the dataset for processing is {2:,}.'.format(dh.num_unique_chars,dh.dataset_size,dh.num_data_windows))\n",
    "\n",
    "# restore saved model for inferencing\n",
    "mh = ModelHelper(epochs=1, tokenizer=tok)\n",
    "new_model = mh.restore_model('weights.20.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate some tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kamala harring down â€” always hase always want to but the reporting and playens and failed the workers of delphi. i always put american workers and\n"
     ]
    }
   ],
   "source": [
    "# Create a long sequence of text\n",
    "print(mh.create_tweet(text='kamala', model=new_model, n_chars=140, temperature=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an updated tweet file with latest 1000 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get the latest 1000 tweets\n",
    "df_latest_tweets = pd.read_json(path_or_buf='https://www.thetrumparchive.com/latest-tweets', orient='records')\n",
    "df_latest_tweets = df_latest_tweets.drop(columns=['isFlagged'])\n",
    "\n",
    "# Sort by date descending\n",
    "df_latest_tweets = df_latest_tweets.sort_values('date')\n",
    "\n",
    "# Cast boolean values as string\n",
    "df_latest_tweets['isRetweet'] = df_latest_tweets['isRetweet'].astype(str)\n",
    "df_latest_tweets['isDeleted'] = df_latest_tweets['isDeleted'].astype(str)\n",
    "\n",
    "# Replace TRUE and FALSE with t and f \n",
    "df_latest_tweets.replace(to_replace='True',value='t',inplace=True)\n",
    "df_latest_tweets.replace(to_replace='False',value='f',inplace=True)\n",
    "\n",
    "# Create consistent datetime format\n",
    "df_latest_tweets['date'] = df_latest_tweets['date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Append to the 11-6-2020 archive\n",
    "df_archive = pd.read_csv('./inputdata/tweets_11-06-2020.csv')\n",
    "df_all_tweets = df_archive.append(df_latest_tweets,ignore_index=True)\n",
    "\n",
    "# Save to a new csv file\n",
    "today = datetime.today().strftime('%m-%d-%Y')\n",
    "file_name = './inputdata/tweets_' + today + '.csv'\n",
    "df_all_tweets.to_csv(file_name,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1000 entries, 999 to 0\n",
      "Data columns (total 8 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         1000 non-null   int64 \n",
      " 1   text       1000 non-null   object\n",
      " 2   isRetweet  1000 non-null   object\n",
      " 3   isDeleted  1000 non-null   object\n",
      " 4   device     1000 non-null   object\n",
      " 5   favorites  1000 non-null   int64 \n",
      " 6   retweets   1000 non-null   int64 \n",
      " 7   date       1000 non-null   object\n",
      "dtypes: int64(3), object(5)\n",
      "memory usage: 70.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_latest_tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
