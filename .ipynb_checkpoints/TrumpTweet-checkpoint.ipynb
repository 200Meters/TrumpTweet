{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"left\">\n",
    "<img src=\"./assets/trump-tweet-sketch.jpg\">\n",
    "</div>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "> **\"Now they (almost all) sit back and watch me fight against a crooked and vicious foe, the radical left democrats. i will never forget!\"**<br>\n",
    "> *-TrumpTweet Bot*\n",
    "\n",
    "### TrumpTweet\n",
    "TrumpTweet is an exploratory project for artificial text generation. Irrespective of one's politics, America's president is an avid and provocative user of Twitter - Tweeting more than 50,000 times as of December 2020! By applying artifical intelligence to model Trump's Tweets one can both gain some insights into Trump's topics of concern and use of language, and how an AI system will learn them in order to generate novel Tweets. In the case of this project, the neural network learns from Trump's past Tweets and will then generate novel Tweets in response to current headlines in the News source of your choice.\n",
    "\n",
    "For example in response to the New York Times January 4, 2021 headline \"Ben Sasse Slams Republican Effort to Challenge Election\" the AI Trump responded with the Tweet \"Now they (almost all) sit back and watch me fight against a crooked and vicious foe, the radical left democrats. i will never forget!\".\n",
    "\n",
    "As another example, Breitbart News ran the headline \"Georgia Election ‘Is All About Divided Government’\" to which the artificial Trump responded \"The presidency of the usa’s 2020 election. ours, with its millions and millions of corrupt mail-in ballots\"\n",
    "\n",
    "#### TrumpTweet Architecture\n",
    "The TrumpTweet project ingests Tweets from TrumpTwitterArchive.com using both the historical archive and the latst 1000 Tweets from Trump. It then uses TensorFlow and Keras to tokenize the Tweets and train a GRU RNN model. A simple set of functions based on Aurelion Geron's novel text generation example are used to create new Tweets. Finally, Python's URLLIB library and BeautifulSoup are used to scrape Google news for headlines from a specified news source. The headlines are then fed to the Tweet Generation routine to create novel Tweets in response to the headlines.\n",
    "\n",
    "This project is not intended as a live web app, though it could certainly be made so if someone wanted to support the cost of the infrastructure. Instead it uses a JupyterLab notebook to orchestrate data ingestion and prep, model training, web scraping, and Tweet creation. The notebook is supported by a python script with several helper functions for data prep, modeling, and Tweeting. \n",
    "\n",
    "#### TrumpTweet Limitations and Possible Enhancements\n",
    "Ideally, the TrumpTweet project would be used live to monitor both Trump's current Tweets, and current headlines and generate an ongoing stream of Tweets in response. However, infrastructure costs would be entailed. Training the GRU model does require GPU hardware in order to complete model training in a reasonable amount of time. This could be done for free at Google Colab for someone willing to re-work the code for a Colab notebook/py script combination, or it could be done on an Azure or AWS machine. The model included took 33 hours to train on Microsoft's most minimal GPU machine with Tensorflow/CuDNN support at a cost of $1.31 per hour. It should be noted that the model was relatively simple, and was only trained for 50 epochs. Due to cost, no effort was made to optimize the model complexity or training depth for more robust responses and realistic Tweets.\n",
    "\n",
    "Some thoughts, therefore, about possible enhancements to the project:\n",
    "\n",
    "- **Model Optimization:** Experimentation with the model may yield more realistic results. The model was trained using only the prior 6 months of Tweets, so  additional training data, a more complex/deeper model, and additional epochs could be experimented with in order to improve results. Additionally, a statefull RNN may be used as well.\n",
    "- **Online Model:** The current architecture does support the ingestion of new data from a secondary API of recent Trump Tweets found at TrumpTwitterArchive.com and does support updating the model and restarting training from a saved model. However, there is no facility for automating this. An improvement would be to create an automated job such as an Azure function to either monitor Twitter directly or the TrumpTwitterArchive, and then update the model on a regular basis to include new Tweets.\n",
    "- **Web App:** Ideally the TrumpTweet project would be published as a web app which would both monitor news feeds and create Tweets in response to headlines, or allow a user to enter their own \"headline\" to which Trump would respond. I also had considered creating a companion BidenTweet model and then have the two of them generate novel Tweets in response to one another, effectively having \"debate\" over Twitter.\n",
    "- **Tweet Trimming:** The current implementation simply generates the next 140 characters. This sometimes cuts off a word, so a feature could be created to trim down to a word in order to end the tweet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Prep\n",
    "Run the following cells to prep the environment. The first cell merely diables warnings that will arise due to the fact the model is set up for two different (CPU and GPU) environments. The second cell is critical as it imports all of the supporting scripts that do the work. Finally, a quick check to see if GPU's are available. If not an empty list is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable warnings just to keep the screen clean\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the helpers scripts with the data and model helper objects\n",
    "%run scripts/helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify GPU support or not\n",
    "tf.config.list_physical_devices('GPU')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a datahelper and process raw data\n",
    "Run the following cell to update the designated archive file with the most recent 1000 Trump Tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive file./inputdata/tweets_01-05-2021.csv has been created\n"
     ]
    }
   ],
   "source": [
    "# Update the archive file if desired. This will retrieve the latest 1000 Tweets and append any not already present to the archive file.\n",
    "# It will then create a new archive file with current date in the name\n",
    "dh = DataHelper(file_name='tweets_12-29-2020.csv')  # Current archive. This is used to establish the DH object\n",
    "dh.update_archive_file(current_archive_file='tweets_01-04-2021.csv')  # File that new tweets will be added to when creating new archive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the archive file is created, designate the time range for which you want to train the model. Note that if you already created the dh object in the cell above, you will want to update the first line with the new archive file name. This code preps the data and creates a clean file with URL's removed from the Tweets since we don't want to train on the URL's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete.\n",
      "The number of Tweets sent by Trump during the period is 3888\n"
     ]
    }
   ],
   "source": [
    "# Create a datahelper object and designate the input file\n",
    "dh = DataHelper(file_name='tweets_12-29-2020.csv')\n",
    "\n",
    "# Prep the raw data to create the tweet file\n",
    "dh.prep_raw_data(start_date='2020-06-01', end_date='2020-12-29')\n",
    "\n",
    "# Print the number of tweets in the time range\n",
    "print('The number of Tweets sent by Trump during the period is {}'.format(dh.num_tweets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize the text and create the dataset for model training\n",
    "In order to do the text analysis, you'll need to tokenize the text. The following line creates a Keras tokenizer object. Note that some of the defaults have been changed on the tokenizer to better handle Tweets. For specifics of the tokenizer, look at the python helper script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset and tokenizer creation complete.\n",
      "The number of unique characters is 101 and the dataset size is 1 document(s). The number of windows in the dataset for processing is 481,083.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text and create the dataset\n",
    "dataset, tokenizer = dh.create_tokenizer('inputdata/clean_tweet.txt')\n",
    "print('The number of unique characters is {0:,} and the dataset size is {1:,} document(s).' \\\n",
    "      ' The number of windows in the dataset for processing is {2:,}.'.format(dh.num_unique_chars,dh.dataset_size,dh.num_data_windows))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the Model and Train It\n",
    "Create the model and train it. Depending on your hardware and how much data you include, it could take a significant amount of time. On my CPU machine, training 3 months of tweets would take more than a month. The GPU machine cut this to a little more than a day.\n",
    "\n",
    "Note that you can adjust some options here like the number of epochs. Also, the model will be saved as a full checkpoint after each epoch in case training gets interrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the modelhelper object\n",
    "mh = ModelHelper(epochs=20)\n",
    "\n",
    "# Create the model\n",
    "model = mh.create_model(tokenizer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam')\n",
    "\n",
    "# Save a checkpoint after every epoch\n",
    "EPOCHS = 20\n",
    "checkpoint_filepath = 'checkpoints/weights.{epoch:02d}.hdf5'\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    save_freq='epoch',\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=False)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(dataset,epochs=EPOCHS,callbacks=[model_checkpoint])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Start Training Model from Saved Checkpoint\n",
    "Training the model takes quite some time. A checkpoint is saved every epoch so the code below will allow you to resume training from a checkpoint. The code below can be used to restart training of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#restart training from saved checkpoint\n",
    "new_model = load_model('checkpoints/weights.09.hdf5')\n",
    "\n",
    "# Save a checkpoint after every epoch\n",
    "EPOCHS = 1\n",
    "checkpoint_filepath = 'checkpoints/weights.{epoch:02d}.hdf5'\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    save_freq='epoch',\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=False)\n",
    "\n",
    "#Fit the model\n",
    "history = new_model.fit(dataset,epochs=EPOCHS,callbacks=[model_checkpoint])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a saved model and generate a Tweet\n",
    "Run the code below to re-load both the data and a saved model in order generate some Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete.\n",
      "The number of Tweets sent by Trump during the period is 3888\n"
     ]
    }
   ],
   "source": [
    "# Create a datahelper object and designate the input file\n",
    "dh = DataHelper(file_name='tweets_12-29-2020.csv')\n",
    "\n",
    "# Prep the raw data to create the tweet file\n",
    "dh.prep_raw_data(start_date='2020-06-01', end_date='2020-12-29')\n",
    "\n",
    "# Print the number of tweets\n",
    "print('The number of Tweets sent by Trump during the period is {}'.format(dh.num_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset and tokenizer creation complete.\n",
      "The number of unique characters is 101 and the dataset size is 1 document(s). The number of windows in the dataset for processing is 481,083.\n"
     ]
    }
   ],
   "source": [
    "# re-create the tokenizer\n",
    "# Tokenize the text and create the dataset\n",
    "dataset, tok = dh.create_tokenizer('inputdata/clean_tweet.txt')\n",
    "print('The number of unique characters is {0:,} and the dataset size is {1:,} document(s).' \\\n",
    "      ' The number of windows in the dataset for processing is {2:,}.'.format(dh.num_unique_chars,dh.dataset_size,dh.num_data_windows))\n",
    "\n",
    "# restore saved model for inferencing\n",
    "mh = ModelHelper(epochs=1, tokenizer=tok)\n",
    "new_model = mh.restore_model('weights.50.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate some tweets\n",
    "The cell below will generate a Tweet based on the passed 'text' argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kamaland and for the presidency, like the democrats would do if they had actually won. the proof is irrefutable! massive late night mail-in ballot\n"
     ]
    }
   ],
   "source": [
    "# Create a long sequence of text\n",
    "print(mh.create_tweet(text='Kamala', model=new_model, n_chars=140, temperature=0.02))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Tweets Based on Current News Headlines\n",
    "If you'd like to see how the artificial Trump would respond to current news headlines, you can run the two cells below. The first cell scrapes some headlines from Google news based on the news source provider entered and returns a list of headlines. The second cell recursively calls the Tweet generator for each headline in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Citrus and Persimmon Salad Recipe - NYT Cooking', 'What Scientists Know About How the Coronavirus Variant Spreads', 'Stimulus Money Should Have Gone to the Jobless, Economists Say', 'Becky Hammon Becomes First Woman to Serve as Head Coach in N.B.A. Game', 'Pope Francis to Skip New Year Services Because of Leg Condition']\n"
     ]
    }
   ],
   "source": [
    "# Get some headlines\n",
    "th = TweetHelper()\n",
    "headlines = th.get_headlines(news_source='nytimes.com', num_headlines=5)\n",
    "print(headlines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline: Citrus and Persimmon Salad Recipe - NYT Cooking\n",
      "Trump Tweet:  at the many reasons it was brought. a rigged election, fight on!\"\n",
      "\"so, you’re the president of the united states\n",
      "\n",
      "full video:  \n",
      "\"they are s\n",
      "-------------------------\n",
      "Headline: What Scientists Know About How the Coronavirus Variant Spreads\n",
      "Trump Tweet:  and allow for removal of military from far away, and very unappreciative, lands. thank you! \n",
      "\"just released data shows many thousands of no\n",
      "-------------------------\n",
      "Headline: Stimulus Money Should Have Gone to the Jobless, Economists Say\n",
      "Trump Tweet:  to “people where are the proof of the people, florida &amp; others are open &amp; doing well. common sense please!\"\n",
      "\"time for republican se\n",
      "-------------------------\n",
      "Headline: Becky Hammon Becomes First Woman to Serve as Head Coach in N.B.A. Game\n",
      "Trump Tweet:  fight for it. don’t let them take it away!\"\n",
      "\"oh, they do this to me every day. when will they apologize? \n",
      "\"governor @briankempga and his pu\n",
      "-------------------------\n",
      "Headline: Pope Francis to Skip New Year Services Because of Leg Condition\n",
      "Trump Tweet: s, and see it wan to stand for our country. show courage, and do what’s right!!! \n",
      "\"i simply wan to get our great people $2000, rather than $\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create some Tweets\n",
    "for headline in headlines:\n",
    "    tweet = mh.create_tweet(text=headline, model=new_model, n_chars=140, temperature=0.02)\n",
    "    tweet = tweet[len(headline):]\n",
    "    print(\"Headline: \" + headline)\n",
    "    print(\"Trump Tweet: \" + tweet)\n",
    "    print(\"-\"*25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
